{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27634864-d12d-435e-8d4d-f32844ece41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from pathlib import Path\n",
    "\n",
    "# import shutil\n",
    "import warnings\n",
    "import opendatasets as od\n",
    "from typing import Optional, Callable, Tuple, Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# import torchvision\n",
    "from torch.utils.data import Subset, random_split\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.coba_dataset import COBA, COBA_Split\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from utils.options import args_parser\n",
    "from utils.train_utils import get_data, get_model\n",
    "from models.Update import LocalUpdate\n",
    "from models.test import test_img\n",
    "import os\n",
    "\n",
    "from utils.options import get_logger\n",
    "from logging import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9696563-e695-4bb6-afa0-97f634d23856",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate the args like in the `main_*.py` files\n",
    "class ARGS:\n",
    "    # federated arguments\n",
    "    # epochs:int = 1000         # rounds of training\n",
    "    epochs: int = 10  # rounds of training\n",
    "    train_test_same: int = 0  # use same testing for\n",
    "    num_users: int = 100  # number of users: K\n",
    "    shard_per_user: int = 2  # classes per user\n",
    "    frac: float = 0.1  # the fraction of clients: C\n",
    "    local_ep: int = 1  # the number of local epochs: E\n",
    "    local_bs: int = 10  # local batch size: B\n",
    "    bs: int = 128  # test batch size\n",
    "    lr: float = 0.01  # learning rate\n",
    "    # results_save:str = \"run1\"\n",
    "    momentum: float = 0.5  # SGD momentum (default: 0.5)\n",
    "    # gpu:int = 0\n",
    "    split: str = \"user\"  # train-test split type, user or sample\n",
    "    # grad_norm:str           # use_gradnorm_avging\n",
    "    local_ep_pretrain: int = 0  # the number of pretrain local ep\n",
    "    lr_decay: float = 1.0  # learning rate decay per round\n",
    "\n",
    "    # model arguments\n",
    "    model: str = \"cnn\"  # model name\n",
    "    kernel_num: int = 9  # number of each kind of kernel\n",
    "    kernel_sizes: str = \"3,4,5\"  # comma-separated kernel size to use for convolution\n",
    "    norm: str = \"batch_norm\"  # batch_norm, layer_norm, or None\n",
    "    num_filters: int = 32  # number of filters for conv nets\n",
    "    max_pool: str = True  # whether use max pooling rather than strided convolutions\n",
    "    num_layers_keep: int = 1  # number layers to keep\n",
    "\n",
    "    # other arguments\n",
    "    dataset: str = \"coba\"  # name of dataset\n",
    "    log_level: str = \"info\"  # level of logger\n",
    "    iid: bool = True  # \"store_true\" #whether iid or not\n",
    "    num_classes: int = 14  # number of classes\n",
    "    num_channels: int = 3  # number of channels of images RGB\n",
    "    gpu: int = 0  # GPU ID, -1 for CPU\n",
    "    stopping_rounds: int = 10  # rounds of early stopping\n",
    "    verbose: bool = True  # \"store_true\"\n",
    "    print_freq: int = 100  # print loss frequency during training\n",
    "    seed: int = 1  # random seed (default:1)\n",
    "    test_freq: int = 1  # how often to test on val set\n",
    "    load_fed: str = \"\"  # define pretrained federated model path\n",
    "    results_save: str = \"run1\"  # define fed results save folder\n",
    "    start_saving: int = 0  # when to start saving models\n",
    "\n",
    "\n",
    "args = ARGS()\n",
    "\n",
    "args.device = torch.device(\n",
    "    \"cuda:{}\".format(args.gpu)\n",
    "    if torch.cuda.is_available() and args.gpu != -1\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "args.num_users, args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4573af6-6dd9-4dc6-a582-4324b9e981dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize CobaDataset\n",
    "coba_dataset: COBA = COBA(root=\"data/coba\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83259c24-1f2b-4403-94b0-6e119d59e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training and testing data -- method 1\n",
    "train_size = int(0.8 * len(coba_dataset))\n",
    "test_size = len(coba_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset=coba_dataset, lengths=[train_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea2319-70bc-463d-ac2f-67b5c93b2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.indices)\n",
    "# label.argmax()\n",
    "# label_encodings[label.argmax().item()]\n",
    "# print(f\"Label: {label_encodings[label.argmax().item()]}\")\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08560e48-1eb9-40a4-99b8-4fa05b5a5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot random train sample\n",
    "example_image = coba_dataset[np.random.choice(train_dataset.indices, 1).item()]\n",
    "img, label = example_image\n",
    "label_encodings = train_dataset.dataset.class_to_idx\n",
    "\n",
    "print(f\"Label: {label_encodings[label.argmax().item()]}\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c41e0f-be5e-40d9-a47e-6048011c7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The random split works!\n",
    "dups = 0\n",
    "for index in test_dataset.indices:\n",
    "    if index in train_dataset.indices:\n",
    "        dups += 1\n",
    "print(dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037c085-5029-4b24-9687-0918f9ec76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sampling import iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d612673-5e15-47b1-ada5-f866fa077238",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users_train = iid(dataset=train_dataset.dataset, args=args)\n",
    "for user, d in dict_users_train.items():\n",
    "    print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b7d7d-b0f9-4878-9763-aa35c2390c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try noniid example\n",
    "import random\n",
    "from utils.sampling import noniid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12220da-6b33-491f-bf5a-996eec479d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users_train, rand_set_all = noniid(dataset=train_dataset.dataset, args=args)\n",
    "for user, d in dict_users_train.items():\n",
    "    print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e640a-ac77-46f4-ace4-9699abafa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa317f4-185d-40a6-955f-582fa30df154",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04262752-e7ed-4c48-a7e9-5d598033c312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6ee77-fa9e-4092-b219-6e3dd062c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\n",
    "    \"cuda:{}\".format(args.gpu)\n",
    "    if torch.cuda.is_available() and args.gpu != -1\n",
    "    else \"cpu\"\n",
    ")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ed093-9054-4fb0-b37f-dee0223faf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop():\n",
    "    filename: str = \"iobt_coba_data_viewer\"\n",
    "    logger: Logger = get_logger(args=args, filename=filename)\n",
    "\n",
    "    logger.log(level=logger.level, msg=f\"Log level: {args.log_level.upper()}\")\n",
    "\n",
    "    args.device = torch.device(\n",
    "        f\"cuda:{args.gpu}\" if torch.cuda.is_available() and args.gpu != -1 else \"cpu\"\n",
    "    )\n",
    "\n",
    "    dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "\n",
    "    logger.debug(\"%s dataset loaded\", args.dataset.upper())\n",
    "\n",
    "    base_dir: Path = Path(\n",
    "        \"save\",\n",
    "        args.dataset,\n",
    "        f\"{args.model}_iid{args.iid}_num{args.num_users}_C{args.frac}_le{args.local_ep}\",\n",
    "        f\"shard{args.shard_per_user}\",\n",
    "    )\n",
    "\n",
    "    run_num: int = int(args.results_save[-1])\n",
    "\n",
    "    for file in base_dir.glob(pattern=\"*\"):\n",
    "        if args.results_save[:-1] in file.as_posix():\n",
    "            run_num += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    args.results_save = f\"{args.results_save[:-1]}{run_num}\"\n",
    "\n",
    "    base_dir = Path(base_dir, args.results_save)\n",
    "\n",
    "    logger.info(\"Base save directory: %s\", base_dir)\n",
    "\n",
    "    if not Path(base_dir, \"fed\").exists():\n",
    "        Path(base_dir, \"fed\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    dict_save_path: Path = Path(base_dir, \"dict_users.pkl\")\n",
    "    with open(dict_save_path, \"wb\") as handle:\n",
    "        pickle.dump((dict_users_train, dict_users_test), handle)\n",
    "\n",
    "    # build model\n",
    "    logger.debug(\"Building Model\")\n",
    "    net_glob = get_model(args)\n",
    "    logger.debug(\"Model built\\n%s\", net_glob)\n",
    "\n",
    "    logger.debug(\"Setting model to training mode\")\n",
    "    net_glob.train()\n",
    "\n",
    "    # training\n",
    "    results_save_path: Path = Path(base_dir, \"fed/results.csv\")\n",
    "\n",
    "    loss_train = []\n",
    "    net_best = None\n",
    "    best_loss = None\n",
    "    best_acc = None\n",
    "    best_epoch = None\n",
    "\n",
    "    w_glob = None\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "\n",
    "    lr: float = args.lr\n",
    "    results: list = []\n",
    "\n",
    "    logger.debug(\"Starting training loop\")\n",
    "    # for _iter in range(args.epochs):\n",
    "    for _iter in range(1):\n",
    "        loss_locals = []\n",
    "        # w_glob = None\n",
    "        # m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "        logger.info(\"Round %3d, lr: %.3f, %s\", _iter, lr, idxs_users)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            logger.debug(\"User %i local training\", idx)\n",
    "            local = LocalUpdate(\n",
    "                args=args, dataset=dataset_train, idxs=dict_users_train[idx]\n",
    "            )\n",
    "            logger.debug(\"\\tcreating net_local\")\n",
    "            net_local = copy.deepcopy(net_glob)\n",
    "            logger.debug(\"\\tnet_local created\")\n",
    "\n",
    "            logger.debug(\"\\ttraining to get w_local and loss\")\n",
    "            w_local, loss = local.train(net=net_local.to(args.device))\n",
    "            logger.debug(\"\\ttraining completed\")\n",
    "\n",
    "            logger.debug(\"\\tadding loss to loss_locals\")\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "            if w_glob is None:\n",
    "                logger.debug(\"\\tcreated w_glob (during User %i)\", idx)\n",
    "                w_glob = copy.deepcopy(w_local)\n",
    "                # for k, tensor in w_glob.items():\n",
    "                #         # w_glob[k] = tensor.detach().cpu()\n",
    "                #         w_glob[k] = tensor.cpu()\n",
    "            else:\n",
    "                logger.debug(\"\\tadding w_local[k] to each key k in w_glob[k]\")\n",
    "                for k in w_glob.keys():\n",
    "                    # w_glob[k] += w_local[k].to(\"cpu\")\n",
    "                    w_glob[k] += w_local[k]\n",
    "\n",
    "        logger.debug(\"Modifying lr\")\n",
    "        lr *= args.lr_decay\n",
    "\n",
    "        # update global weights\n",
    "        logger.debug(\"Updating global weights\")\n",
    "        for k in w_glob.keys():\n",
    "            w_glob[k] = torch.div(w_glob[k], m)\n",
    "\n",
    "        # copy weight to net_glob\n",
    "        logger.debug(\"Copying weights\")\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "        logger.debug(\"Calculating Loss\")\n",
    "        loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "        loss_train.append(loss_avg)\n",
    "\n",
    "        if (_iter + 1) % args.test_freq == 0:\n",
    "            logger.debug(\"Evaluating net_glob\")\n",
    "            net_glob.eval()\n",
    "\n",
    "            # pylint: disable=unbalanced-tuple-unpacking\n",
    "            logger.debug(\"Calculating acc_test and loss_test\")\n",
    "            # acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            acc_test, loss_test, f1_test, precision_test, recall_test = test_img(\n",
    "                net_glob, dataset_test, args\n",
    "            )\n",
    "            logger.info(\n",
    "                # \"\\tRound %3d, Avg loss %.3f, Test loss %.6f, Test accuracy: %.2f\",\n",
    "                \"\\tAvg loss %.3f, Test loss %.6f, Test accuracy: %.2f\",\n",
    "                # _iter,\n",
    "                loss_avg,\n",
    "                loss_test,\n",
    "                acc_test,\n",
    "            )\n",
    "\n",
    "            if best_acc is None or acc_test > best_acc:\n",
    "                net_best = copy.deepcopy(net_glob)\n",
    "                best_acc = acc_test\n",
    "                best_epoch = _iter\n",
    "\n",
    "            # if (iter + 1) > args.start_saving:\n",
    "            #     model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(_iter + 1))\n",
    "            #     torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "            results.append(np.array([_iter, loss_avg, loss_test, acc_test, best_acc]))\n",
    "            final_results = np.array(results)\n",
    "            final_results = pd.DataFrame(\n",
    "                final_results,\n",
    "                columns=[\"epoch\", \"loss_avg\", \"loss_test\", \"acc_test\", \"best_acc\"],\n",
    "            )\n",
    "            final_results.to_csv(results_save_path, index=False)\n",
    "\n",
    "        if (_iter + 1) % 50 == 0:\n",
    "            best_save_path: Path = Path(base_dir, f\"fed/best_{_iter+1}.pt\")\n",
    "            model_save_path: Path = Path(base_dir, f\"fed/model_{_iter+1}.pt\")\n",
    "\n",
    "            if args.device.type != \"cpu\":\n",
    "                torch.save(\n",
    "                    net_best.to(torch.device(\"cpu\")).state_dict(), best_save_path\n",
    "                )\n",
    "                torch.save(\n",
    "                    net_glob.to(torch.device(\"cpu\")).state_dict(), model_save_path\n",
    "                )\n",
    "            else:\n",
    "                torch.save(net_best.state_dict(), best_save_path)\n",
    "                torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "    logger.info(\"Best model, iter: %i, acc: %f\", best_epoch, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ef2db-66da-4ff1-9964-5809fd1a6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fef1b9-bcea-4ed7-a5e8-daf8027b0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Troubleshooting with GPU errors\n",
    "# print(\"Device before\")\n",
    "# for k in w_glob.keys():\n",
    "#     print(f\"\\t {w_glob[k].device}\")\n",
    "\n",
    "# w_glob = copy.deepcopy(w_local)\n",
    "\n",
    "# for k,value in w_glob.items():\n",
    "#     # w_glob[k] = value.detach().cpu()\n",
    "#     w_glob[k] = value.cpu()\n",
    "\n",
    "# print(\"Device after\")\n",
    "# for k in w_glob.keys():\n",
    "#     print(f\"\\t {w_glob[k].device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Troubleshooting with Shape errors\n",
    "# image,label = coba_dataset[0]\n",
    "# print(image.shape)\n",
    "# image = image.permute(2,0,1)\n",
    "# print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868ce58-d622-4776-8890-9b78719ff5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs, labels = train_dataset.dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373be083-4e88-45f7-a354-78c3ed1cf0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896788bc-a44c-460d-85d7-8862a78aa318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(list(map(torch.argmax,labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9f58e-e016-46db-ae4d-93a0e7ff3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853c84a-9746-441d-b5d5-165335b63ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d207ade-dca8-422e-a275-2c5e413f5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(test_dataset.dataset.class_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808e1b4-0c11-4ae9-b678-5975fe281312",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mnist = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "args.num_users = 100\n",
    "args.num_classes = 10\n",
    "args.dataset = \"mnist\"\n",
    "args.model = \"mlp\"\n",
    "\n",
    "dataset_train = datasets.MNIST(\n",
    "    \"data/mnist/\", train=True, download=True, transform=trans_mnist\n",
    ")\n",
    "dataset_test = datasets.MNIST(\n",
    "    \"data/mnist/\", train=False, download=True, transform=trans_mnist\n",
    ")\n",
    "\n",
    "dict_users_train, rand_set_all = noniid(dataset=dataset_train, args=args)\n",
    "dict_users_test, rand_set_all = noniid(\n",
    "    dataset=dataset_test,\n",
    "    args=args,\n",
    "    rand_set_all=rand_set_all,\n",
    ")\n",
    "main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36973648-4f87-4268-9b45-08d35719a138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_cifar10_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "trans_cifar10_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "args.num_users = 100\n",
    "args.num_classes = 10\n",
    "args.dataset = \"cifar10\"\n",
    "args.model = \"cnn\"\n",
    "\n",
    "dataset_train = datasets.CIFAR10(\n",
    "    \"data/cifar10/\", train=True, download=True, transform=trans_cifar10_train\n",
    ")\n",
    "dataset_test = datasets.CIFAR10(\n",
    "    \"data/cifar10/\", train=False, download=True, transform=trans_cifar10_val\n",
    ")\n",
    "\n",
    "dict_users_train, rand_set_all = noniid(dataset=dataset_train, args=args)\n",
    "dict_users_test, rand_set_all = noniid(\n",
    "    dataset=dataset_test,\n",
    "    args=args,\n",
    "    rand_set_all=rand_set_all,\n",
    ")\n",
    "main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e39c17-60dd-4f56-b22f-a938e598eaa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Simulate the args like in the `main_*.py` files\n",
    "class ARGS:\n",
    "    # federated arguments\n",
    "    # epochs:int = 1000         # rounds of training\n",
    "    epochs: int = 10  # rounds of training\n",
    "    train_test_same: int = 0  # use same testing for\n",
    "    num_users: int = 100  # number of users: K\n",
    "    shard_per_user: int = 2  # classes per user\n",
    "    frac: float = 0.1  # the fraction of clients: C\n",
    "    local_ep: int = 1  # the number of local epochs: E\n",
    "    local_bs: int = 10  # local batch size: B\n",
    "    bs: int = 128  # test batch size\n",
    "    lr: float = 0.01  # learning rate\n",
    "    # results_save:str = \"run1\"\n",
    "    momentum: float = 0.5  # SGD momentum (default: 0.5)\n",
    "    # gpu:int = 0\n",
    "    split: str = \"user\"  # train-test split type, user or sample\n",
    "    # grad_norm:str           # use_gradnorm_avging\n",
    "    local_ep_pretrain: int = 0  # the number of pretrain local ep\n",
    "    lr_decay: float = 1.0  # learning rate decay per round\n",
    "\n",
    "    # model arguments\n",
    "    model: str = \"cnn\"  # model name\n",
    "    kernel_num: int = 9  # number of each kind of kernel\n",
    "    kernel_sizes: str = \"3,4,5\"  # comma-separated kernel size to use for convolution\n",
    "    norm: str = \"batch_norm\"  # batch_norm, layer_norm, or None\n",
    "    num_filters: int = 32  # number of filters for conv nets\n",
    "    max_pool: str = True  # whether use max pooling rather than strided convolutions\n",
    "    num_layers_keep: int = 1  # number layers to keep\n",
    "\n",
    "    # other arguments\n",
    "    dataset: str = \"coba\"  # name of dataset\n",
    "    log_level: str = \"info\"  # level of logger\n",
    "    iid: bool = True  # \"store_true\" #whether iid or not\n",
    "    num_classes: int = 14  # number of classes\n",
    "    num_channels: int = 3  # number of channels of images RGB\n",
    "    gpu: int = 0  # GPU ID, -1 for CPU\n",
    "    stopping_rounds: int = 10  # rounds of early stopping\n",
    "    verbose: bool = True  # \"store_true\"\n",
    "    print_freq: int = 100  # print loss frequency during training\n",
    "    seed: int = 1  # random seed (default:1)\n",
    "    test_freq: int = 1  # how often to test on val set\n",
    "    load_fed: str = \"\"  # define pretrained federated model path\n",
    "    results_save: str = \"run1\"  # define fed results save folder\n",
    "    start_saving: int = 0  # when to start saving models\n",
    "\n",
    "\n",
    "args = ARGS()\n",
    "\n",
    "args.device = torch.device(\n",
    "    \"cuda:{}\".format(args.gpu)\n",
    "    if torch.cuda.is_available() and args.gpu != -1\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "args.num_users, args.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43665c55-63d5-4ec5-93a1-19d19680367c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Verify Train/Test data differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc09ae-9bb4-4923-b393-4a1870db1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_dataset: COBA = COBA(root=\"data/coba\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13bc82-4b09-4bbe-b53a-56de88ae1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size: int = int(\n",
    "#             0.8 * len(coba_dataset)\n",
    "# )  # maybe TODO: make the percentage customizable (part of `args`)\n",
    "# test_size: int = len(coba_dataset) - train_size\n",
    "# dataset_train, dataset_test = random_split(\n",
    "#     dataset=coba_dataset, lengths=[train_size, test_size]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1099d-cc17-40ba-a310-43cb3ce7ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf8ad2-1b1f-48f0-b17e-42e07d6f33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset_train = [(img,label) for img,label in dataset_train]\n",
    "# len(new_dataset_train) == train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7401b2d-0f7a-47b5-8b4e-63a649b9b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_train = COBA_Split(dataset=dataset_train)\n",
    "# d_test = COBA_Split(dataset=dataset_test)\n",
    "\n",
    "# len(d_train), len(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1201e5-1867-4140-9618-7f4b37335e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iid\n",
    "# dict_users_train: Dict[int, set] = iid(\n",
    "#                 dataset=d_train, args=args\n",
    "# )\n",
    "# dict_users_test: Dict[int, set] = iid(\n",
    "#     dataset=d_test, args=args\n",
    "# )\n",
    "# for user,d in dict_users_train.items():\n",
    "#     print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc61a0-bcbe-4301-941d-d17e007df7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # noniid\n",
    "# dict_users_train, rand_set_all = noniid(\n",
    "#                 dataset=d_train, args=args\n",
    "# )\n",
    "# dict_users_test, rand_set_all = noniid(\n",
    "#     dataset=d_test,\n",
    "#     args=args,\n",
    "#     rand_set_all=rand_set_all,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ea8c2-a8e6-4d75-a126-f69b70dcb06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for user,d in dict_users_train.items():\n",
    "#     print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71279c16-d47e-45c3-8ab6-ea8497e314ad",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76118530-511b-4eac-822d-b3dcb0d34b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\n",
    "    \"cuda:{}\".format(args.gpu)\n",
    "    if torch.cuda.is_available() and args.gpu != -1\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "\n",
    "model = get_model(args)\n",
    "\n",
    "model_state_dict_path: str = Path(\n",
    "    \"save\",\n",
    "    \"coba_legacy\",\n",
    "    \"cnn_iidFalse_num98_C0.3_le1\",\n",
    "    \"shard2\",\n",
    "    \"seed10_coba_fedavg_bestcase_run12\",\n",
    "    \"fed\",\n",
    "    \"model_1000.pt\",\n",
    ")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(model_state_dict_path)\n",
    ") if args.device.type != \"cpu\" else model.load_state_dict(\n",
    "    torch.load(model_state_dict_path, map_location=torch.device(\"cpu\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1ae0d-6cc3-4b5c-abbf-30861b24e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader: DataLoader = DataLoader(dataset_test, batch_size=args.bs)\n",
    "IS_USING_GPU: bool = args.gpu != -1 and args.device.type != \"cpu\"\n",
    "\n",
    "probs: np.array = np.array([])\n",
    "y_preds: np.array = np.array([])\n",
    "y_trues: np.array = np.array([])\n",
    "# coba_cms = []\n",
    "\n",
    "for _, (data, target) in enumerate(data_loader):\n",
    "    if args.gpu != -1 and args.device.type != \"cpu\":\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "    if args.dataset == \"coba\":\n",
    "        data = data.permute(0, 3, 1, 2)\n",
    "\n",
    "    log_probs: torch.Tensor = model(data)\n",
    "\n",
    "    probs: np.array = (\n",
    "        np.append(probs, log_probs.cpu().data.numpy())\n",
    "        if IS_USING_GPU\n",
    "        else np.append(probs, log_probs.data.numpy())\n",
    "    )\n",
    "\n",
    "    y_pred: torch.Tensor = (\n",
    "        log_probs.cpu().data.max(1, keepdim=True)[1]\n",
    "        if args.device.type != \"cpu\"\n",
    "        else log_probs.data.max(1, keepdim=True)[1]\n",
    "    )\n",
    "\n",
    "    y_true: torch.Tensor = (\n",
    "        torch.tensor(list(map(torch.argmax, target.data)), device=\"cpu\").data.view_as(\n",
    "            y_pred\n",
    "        )\n",
    "        if args.dataset == \"coba\"\n",
    "        else target.to(\"cpu\").data.view_as(y_pred)\n",
    "    )\n",
    "\n",
    "    # coba_cms.append(confusion_matrix(y_pred=y_pred,y_true=y_true))\n",
    "\n",
    "    y_preds = np.append(y_preds, y_pred)\n",
    "    y_trues = np.append(y_trues, y_true)\n",
    "\n",
    "y_preds.shape == y_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e08be5-8b91-496d-8264-e975eef9dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_cm = confusion_matrix(y_pred=y_preds, y_true=y_trues)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=coba_cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6734b-6159-444b-9c56-d046a88a46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba_cm_gen = (cm for cm in coba_cms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f73eea-d6cb-42ff-a087-851825603957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disp = ConfusionMatrixDisplay(confusion_matrix=next(coba_cm_gen))\n",
    "# disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb9d2d-d92a-44e0-987d-9404d63285e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(coba_cms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18271eee-4ca5-4ff4-be3c-59aeb3b9712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, target in data_loader:\n",
    "#     if args.dataset == \"coba\":\n",
    "#         data = data.permute(0, 3, 1, 2) # rearranged to be: (batch size: N, color channels: C, height: H, width: W)\n",
    "\n",
    "#     print(data.shape)\n",
    "#     print(target.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0f86a-742f-4d62-9d44-cdc57c16f9e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Check class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9868e6-c828-4195-a0f8-92a2b31c2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_dataset: COBA = COBA(root=\"data/coba\", download=True)\n",
    "all_targets: np.array = np.array(list(map(torch.argmax, coba_dataset.targets)))\n",
    "all_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da81df-25f4-4241-bf50-1ca5f9d9880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class, target_occurences = np.unique(all_targets, return_counts=True)\n",
    "# target_class, target_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8503e-4dd4-4da7-8ca6-f40ea2d5b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = coba_dataset.class_to_idx\n",
    "for tc, occurence in zip(target_class, target_occurences):\n",
    "    print(f\"{class_to_idx[tc]:<13} ({tc:2}):\\t{occurence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfa956-7204-4a4b-90d1-d5ffec1510f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_spread_df = pd.DataFrame(\n",
    "    {\n",
    "        \"occurences\": target_occurences,\n",
    "        \"classes\": [class_to_idx[tc].title() for tc in target_class],\n",
    "    }\n",
    ")\n",
    "plt.xlabel(\"Occurences\")\n",
    "plt.ylabel(\"Classes\")\n",
    "sns.barplot(data=target_spread_df, y=\"classes\", x=\"occurences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46192851-7653-498d-9658-42de77a22b23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee3bb6-07d5-404d-9a8a-044605488c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_results_path: str = Path(\n",
    "    \"save\",\n",
    "    \"coba_legacy\",\n",
    "    \"cnn_iidFalse_num98_C0.3_le1\",\n",
    "    \"shard2\",\n",
    "    \"seed10_coba_fedavg_bestcase_run12\",\n",
    "    \"fed\",\n",
    "    \"results.csv\",\n",
    ")\n",
    "coba_results_df: pd.DataFrame = pd.read_csv(coba_results_path, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc6212-f54d-4239-8045-999a98804fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_results_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec346389-064c-46db-a948-8cda6741575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_adjusted(col_name: str, df: pd.DataFrame) -> None:\n",
    "    adjusted_col = (\n",
    "        df.groupby(np.arange(len(df)) // 10).mean()[col_name].values\n",
    "    )  # averaged every 10 epochs\n",
    "    epochs = np.arange(len(df) // 10) * 10\n",
    "    sns.lineplot(x=epochs, y=adjusted_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb9f0d-dabf-4d64-a38b-63ebbfd187af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fbc9d-7b17-4734-9468-4020fbf7d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_accs:np.array = coba_results_df[\"best_acc\"].values\n",
    "# sns.lineplot(data=coba_results_df, x=\"epoch\", y=\"best_acc\")\n",
    "\n",
    "graph_adjusted(col_name=\"best_acc\", df=coba_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9ef68-a2f7-4d57-9c93-58ee58ea84bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4978bb6-ec01-416d-a065-b6910e9add14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = coba_results_df[\"epoch\"].values\n",
    "# acc_test = coba_results_df[\"acc_test\"].values\n",
    "\n",
    "graph_adjusted(col_name=\"acc_test\", df=coba_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a47e22-9f24-403d-a668-83e222f22a6f",
   "metadata": {},
   "source": [
    "### Loss (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50483e4-c8d7-4e95-a0df-2beebef7726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = coba_results_df[\"epoch\"].values\n",
    "# loss_test = coba_results_df[\"loss_test\"].values\n",
    "\n",
    "graph_adjusted(col_name=\"loss_test\", df=coba_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10bc64-2920-4652-a28c-98ef86bf728c",
   "metadata": {},
   "source": [
    "### Loss (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13a3de-0cbc-493a-aad7-687806436909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = coba_results_df[\"epoch\"].values\n",
    "# loss_avg = coba_results_df[\"loss_avg\"].values\n",
    "\n",
    "graph_adjusted(col_name=\"loss_avg\", df=coba_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd5518-805b-450a-9bde-99737968ab8b",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc15d3-4ff1-4d97-8915-38b3b8274fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = coba_results_df[\"epoch\"].values\n",
    "# f1_test = coba_results_df[\"f1_test\"].values\n",
    "\n",
    "graph_adjusted(col_name=\"f1_test\", df=coba_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1de95-08b6-4a96-8e2e-6efa65156885",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5ddcc-237d-4573-a9d1-8aa59dc08e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = coba_results_df[\"epoch\"].values\n",
    "# precision_test = coba_results_df[\"precision_test\"].values\n",
    "\n",
    "graph_adjusted(col_name=\"precision_test\", df=coba_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e0824-a0af-4857-803c-a022dbab4f68",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc797a58-b7f1-4274-8197-73419b37f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = coba_results_df[\"epoch\"].values\n",
    "# recall_test = coba_results_df[\"recall_test\"].values\n",
    "\n",
    "graph_adjusted(col_name=\"recall_test\", df=coba_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
