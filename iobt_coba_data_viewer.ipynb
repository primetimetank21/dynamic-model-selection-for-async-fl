{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27634864-d12d-435e-8d4d-f32844ece41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "# import os\n",
    "from pathlib import Path\n",
    "# import shutil\n",
    "import warnings\n",
    "import opendatasets as od\n",
    "from typing import Optional, Callable, Tuple, Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# import torchvision\n",
    "from torch.utils.data import Subset, random_split\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9696563-e695-4bb6-afa0-97f634d23856",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate the args like in the `main_*.py` files\n",
    "class ARGS:\n",
    "    #federated arguments\n",
    "    # epochs:int = 1000         # rounds of training\n",
    "    epochs:int = 10           # rounds of training\n",
    "    num_users:int = 100       # number of users: K\n",
    "    shard_per_user:int = 2    # classes per user\n",
    "    frac:float = 0.1          # the fraction of clients: C\n",
    "    local_ep:int = 1          # the number of local epochs: E\n",
    "    local_bs:int = 10         # local batch size: B\n",
    "    bs:int = 128              # test batch size\n",
    "    lr:float = 0.01           # learning rate\n",
    "    # results_save:str = \"run1\"\n",
    "    momentum:float = 0.5      # SGD momentum (default: 0.5)\n",
    "    # gpu:int = 0\n",
    "    split:str = \"user\"        # train-test split type, user or sample\n",
    "    # grad_norm:str           # use_gradnorm_avging\n",
    "    local_ep_pretrain:int = 0 # the number of pretrain local ep\n",
    "    lr_decay:float = 1.0      # learning rate decay per round\n",
    "\n",
    "    # model arguments\n",
    "    model:str = \"cnn\"          # model name\n",
    "    kernel_num:int = 9         # number of each kind of kernel\n",
    "    kernel_sizes:str = \"3,4,5\" # comma-separated kernel size to use for convolution\n",
    "    norm:str = \"batch_norm\"    # batch_norm, layer_norm, or None\n",
    "    num_filters:int = 32       # number of filters for conv nets\n",
    "    max_pool:str = True        # whether use max pooling rather than strided convolutions\n",
    "    num_layers_keep:int = 1    # number layers to keep\n",
    "    \n",
    "    # other arguments\n",
    "    dataset:str = \"coba\"      # name of dataset\n",
    "    log_level:str = \"info\"    # level of logger\n",
    "    iid:bool = True           # \"store_true\" #whether iid or not\n",
    "    num_classes:int = 14      # number of classes\n",
    "    num_channels:int = 3      # number of channels of images RGB\n",
    "    gpu:int = 0               # GPU ID, -1 for CPU\n",
    "    stopping_rounds:int = 10  # rounds of early stopping\n",
    "    verbose:bool = True       # \"store_true\"\n",
    "    print_freq:int = 100      # print loss frequency during training\n",
    "    seed:int = 1              # random seed (default:1)\n",
    "    test_freq:int = 1         # how often to test on val set\n",
    "    load_fed:str = \"\"         # define pretrained federated model path\n",
    "    results_save:str = \"run1\" # define fed results save folder\n",
    "    start_saving:int = 0      # when to start saving models\n",
    "\n",
    "\n",
    "args = ARGS()\n",
    "args.num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4573af6-6dd9-4dc6-a582-4324b9e981dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.coba_dataset import COBA\n",
    "\n",
    "## Initialize CobaDataset\n",
    "coba_dataset = COBA(root=\"data/coba\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83259c24-1f2b-4403-94b0-6e119d59e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training and testing data -- method 1\n",
    "train_size = int(0.8 * len(coba_dataset))\n",
    "test_size = len(coba_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset=coba_dataset, lengths=[train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea2319-70bc-463d-ac2f-67b5c93b2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.indices)\n",
    "# label.argmax()\n",
    "# label_encodings[label.argmax().item()]\n",
    "# print(f\"Label: {label_encodings[label.argmax().item()]}\")\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08560e48-1eb9-40a4-99b8-4fa05b5a5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot random train sample\n",
    "example_image = coba_dataset[np.random.choice(train_dataset.indices, 1).item()]\n",
    "img, label = example_image\n",
    "label_encodings = train_dataset.dataset.class_to_idx\n",
    "\n",
    "print(f\"Label: {label_encodings[label.argmax().item()]}\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c41e0f-be5e-40d9-a47e-6048011c7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The random split works!\n",
    "dups = 0\n",
    "for index in test_dataset.indices:\n",
    "    if index in train_dataset.indices:\n",
    "        dups += 1\n",
    "print(dups)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037c085-5029-4b24-9687-0918f9ec76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sampling import iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d612673-5e15-47b1-ada5-f866fa077238",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users_train = iid(dataset=train_dataset.dataset, args=args)\n",
    "for user,d in dict_users_train.items():\n",
    "    print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b7d7d-b0f9-4878-9763-aa35c2390c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try noniid example\n",
    "import random\n",
    "from utils.sampling import noniid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12220da-6b33-491f-bf5a-996eec479d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users_train, rand_set_all = noniid(dataset=train_dataset.dataset, args=args)\n",
    "for user,d in dict_users_train.items():\n",
    "    print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e640a-ac77-46f4-ace4-9699abafa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa317f4-185d-40a6-955f-582fa30df154",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04262752-e7ed-4c48-a7e9-5d598033c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from utils.options import args_parser\n",
    "from utils.train_utils import get_data, get_model\n",
    "from models.Update import LocalUpdate\n",
    "from models.test import test_img\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6ee77-fa9e-4092-b219-6e3dd062c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\n",
    "        \"cuda:{}\".format(args.gpu)\n",
    "        if torch.cuda.is_available() and args.gpu != -1\n",
    "        else \"cpu\"\n",
    ")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ed093-9054-4fb0-b37f-dee0223faf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.options import get_logger\n",
    "from logging import Logger\n",
    "def main_loop():\n",
    "    filename: str = \"iobt_coba_data_viewer\"\n",
    "    logger: Logger = get_logger(args=args, filename=filename)\n",
    "    \n",
    "    logger.log(level=logger.level, msg=f\"Log level: {args.log_level.upper()}\")\n",
    "    \n",
    "    args.device = torch.device(\n",
    "        f\"cuda:{args.gpu}\" if torch.cuda.is_available() and args.gpu != -1 else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "        \n",
    "    logger.debug(\"%s dataset loaded\", args.dataset.upper())\n",
    "    \n",
    "    base_dir: Path = Path(\n",
    "        \"save\",\n",
    "        args.dataset,\n",
    "        f\"{args.model}_iid{args.iid}_num{args.num_users}_C{args.frac}_le{args.local_ep}\",\n",
    "        f\"shard{args.shard_per_user}\",\n",
    "    )\n",
    "    \n",
    "    run_num: int = int(args.results_save[-1])\n",
    "    \n",
    "    for file in base_dir.glob(pattern=\"*\"):\n",
    "        if args.results_save[:-1] in file.as_posix():\n",
    "            run_num += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    args.results_save = f\"{args.results_save[:-1]}{run_num}\"\n",
    "    \n",
    "    base_dir = Path(base_dir, args.results_save)\n",
    "    \n",
    "    logger.info(\"Base save directory: %s\", base_dir)\n",
    "    \n",
    "    if not Path(base_dir, \"fed\").exists():\n",
    "        Path(base_dir, \"fed\").mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    dict_save_path: Path = Path(base_dir, \"dict_users.pkl\")\n",
    "    with open(dict_save_path, \"wb\") as handle:\n",
    "        pickle.dump((dict_users_train, dict_users_test), handle)\n",
    "    \n",
    "    # build model\n",
    "    logger.debug(\"Building Model\")\n",
    "    net_glob = get_model(args)\n",
    "    logger.debug(\"Model built\\n%s\", net_glob)\n",
    "    \n",
    "    logger.debug(\"Setting model to training mode\")\n",
    "    net_glob.train()\n",
    "    \n",
    "    # training\n",
    "    results_save_path: Path = Path(base_dir, \"fed/results.csv\")\n",
    "    \n",
    "    loss_train = []\n",
    "    net_best = None\n",
    "    best_loss = None\n",
    "    best_acc = None\n",
    "    best_epoch = None\n",
    "    \n",
    "    w_glob = None\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    \n",
    "    lr: float = args.lr\n",
    "    results: list = []\n",
    "    \n",
    "    logger.debug(\"Starting training loop\")\n",
    "    # for _iter in range(args.epochs):\n",
    "    for _iter in range(1):\n",
    "        loss_locals = []\n",
    "        # w_glob = None\n",
    "        # m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "        logger.info(\"Round %3d, lr: %.3f, %s\", _iter, lr, idxs_users)\n",
    "    \n",
    "        for idx in idxs_users:\n",
    "            logger.debug(\"User %i local training\", idx)\n",
    "            local = LocalUpdate(\n",
    "                args=args, dataset=dataset_train, idxs=dict_users_train[idx]\n",
    "            )\n",
    "            logger.debug(\"\\tcreating net_local\")\n",
    "            net_local = copy.deepcopy(net_glob)\n",
    "            logger.debug(\"\\tnet_local created\")\n",
    "    \n",
    "            logger.debug(\"\\ttraining to get w_local and loss\")\n",
    "            w_local, loss = local.train(net=net_local.to(args.device))\n",
    "            logger.debug(\"\\ttraining completed\")\n",
    "    \n",
    "            logger.debug(\"\\tadding loss to loss_locals\")\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "    \n",
    "            if w_glob is None:\n",
    "                logger.debug(\"\\tcreated w_glob (during User %i)\", idx)\n",
    "                w_glob = copy.deepcopy(w_local)\n",
    "                # for k, tensor in w_glob.items():\n",
    "                #         # w_glob[k] = tensor.detach().cpu()\n",
    "                #         w_glob[k] = tensor.cpu()\n",
    "            else:\n",
    "                logger.debug(\"\\tadding w_local[k] to each key k in w_glob[k]\")\n",
    "                for k in w_glob.keys():\n",
    "                    # w_glob[k] += w_local[k].to(\"cpu\")\n",
    "                    w_glob[k] += w_local[k]\n",
    "    \n",
    "        logger.debug(\"Modifying lr\")\n",
    "        lr *= args.lr_decay\n",
    "    \n",
    "        # update global weights\n",
    "        logger.debug(\"Updating global weights\")\n",
    "        for k in w_glob.keys():\n",
    "            w_glob[k] = torch.div(w_glob[k], m)\n",
    "    \n",
    "        # copy weight to net_glob\n",
    "        logger.debug(\"Copying weights\")\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "        # print loss\n",
    "        logger.debug(\"Calculating Loss\")\n",
    "        loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "        loss_train.append(loss_avg)\n",
    "    \n",
    "        if (_iter + 1) % args.test_freq == 0:\n",
    "            logger.debug(\"Evaluating net_glob\")\n",
    "            net_glob.eval()\n",
    "    \n",
    "            # pylint: disable=unbalanced-tuple-unpacking\n",
    "            logger.debug(\"Calculating acc_test and loss_test\")\n",
    "            # acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            acc_test, loss_test, f1_test, precision_test, recall_test = test_img(net_glob, dataset_test, args)\n",
    "            logger.info(\n",
    "                # \"\\tRound %3d, Avg loss %.3f, Test loss %.6f, Test accuracy: %.2f\",\n",
    "                \"\\tAvg loss %.3f, Test loss %.6f, Test accuracy: %.2f\",\n",
    "                # _iter,\n",
    "                loss_avg,\n",
    "                loss_test,\n",
    "                acc_test,\n",
    "            )\n",
    "    \n",
    "            if best_acc is None or acc_test > best_acc:\n",
    "                net_best = copy.deepcopy(net_glob)\n",
    "                best_acc = acc_test\n",
    "                best_epoch = _iter\n",
    "    \n",
    "            # if (iter + 1) > args.start_saving:\n",
    "            #     model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(_iter + 1))\n",
    "            #     torch.save(net_glob.state_dict(), model_save_path)\n",
    "    \n",
    "            results.append(np.array([_iter, loss_avg, loss_test, acc_test, best_acc]))\n",
    "            final_results = np.array(results)\n",
    "            final_results = pd.DataFrame(\n",
    "                final_results,\n",
    "                columns=[\"epoch\", \"loss_avg\", \"loss_test\", \"acc_test\", \"best_acc\"],\n",
    "            )\n",
    "            final_results.to_csv(results_save_path, index=False)\n",
    "    \n",
    "        if (_iter + 1) % 50 == 0:\n",
    "            best_save_path: Path = Path(base_dir, f\"fed/best_{_iter+1}.pt\")\n",
    "            model_save_path: Path = Path(base_dir, f\"fed/model_{_iter+1}.pt\")\n",
    "    \n",
    "            if args.device.type != \"cpu\":\n",
    "                torch.save(\n",
    "                    net_best.to(torch.device(\"cpu\")).state_dict(), best_save_path\n",
    "                )\n",
    "                torch.save(\n",
    "                    net_glob.to(torch.device(\"cpu\")).state_dict(), model_save_path\n",
    "                )\n",
    "            else:\n",
    "                torch.save(net_best.state_dict(), best_save_path)\n",
    "                torch.save(net_glob.state_dict(), model_save_path)\n",
    "    \n",
    "    logger.info(\"Best model, iter: %i, acc: %f\", best_epoch, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ef2db-66da-4ff1-9964-5809fd1a6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fef1b9-bcea-4ed7-a5e8-daf8027b0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Troubleshooting with GPU errors\n",
    "# print(\"Device before\")\n",
    "# for k in w_glob.keys():\n",
    "#     print(f\"\\t {w_glob[k].device}\")\n",
    "    \n",
    "# w_glob = copy.deepcopy(w_local)\n",
    "\n",
    "# for k,value in w_glob.items():\n",
    "#     # w_glob[k] = value.detach().cpu()\n",
    "#     w_glob[k] = value.cpu()\n",
    "\n",
    "# print(\"Device after\")\n",
    "# for k in w_glob.keys():\n",
    "#     print(f\"\\t {w_glob[k].device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Troubleshooting with Shape errors\n",
    "# image,label = coba_dataset[0]\n",
    "# print(image.shape)\n",
    "# image = image.permute(2,0,1)\n",
    "# print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868ce58-d622-4776-8890-9b78719ff5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = train_dataset.dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373be083-4e88-45f7-a354-78c3ed1cf0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896788bc-a44c-460d-85d7-8862a78aa318",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(list(map(torch.argmax,labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9f58e-e016-46db-ae4d-93a0e7ff3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853c84a-9746-441d-b5d5-165335b63ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d207ade-dca8-422e-a275-2c5e413f5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_dataset.dataset.class_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808e1b4-0c11-4ae9-b678-5975fe281312",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mnist = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "args.num_users = 100\n",
    "args.num_classes = 10\n",
    "args.dataset = \"mnist\"\n",
    "args.model = \"mlp\"\n",
    "\n",
    "dataset_train = datasets.MNIST(\n",
    "            \"data/mnist/\", train=True, download=True, transform=trans_mnist\n",
    "        )\n",
    "dataset_test = datasets.MNIST(\n",
    "            \"data/mnist/\", train=False, download=True, transform=trans_mnist\n",
    "        )\n",
    "\n",
    "dict_users_train, rand_set_all = noniid(dataset=dataset_train, args=args)\n",
    "dict_users_test, rand_set_all = noniid(\n",
    "                dataset=dataset_test,\n",
    "                args=args,\n",
    "                rand_set_all=rand_set_all,\n",
    "            )\n",
    "main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36973648-4f87-4268-9b45-08d35719a138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_cifar10_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "trans_cifar10_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "args.num_users = 100\n",
    "args.num_classes = 10\n",
    "args.dataset = \"cifar10\"\n",
    "args.model = \"cnn\"\n",
    "\n",
    "dataset_train = datasets.CIFAR10(\n",
    "            \"data/cifar10/\", train=True, download=True, transform=trans_cifar10_train)\n",
    "dataset_test = datasets.CIFAR10(\n",
    "            \"data/cifar10/\", train=False, download=True, transform=trans_cifar10_val)\n",
    "\n",
    "dict_users_train, rand_set_all = noniid(dataset=dataset_train, args=args)\n",
    "dict_users_test, rand_set_all = noniid(\n",
    "                dataset=dataset_test,\n",
    "                args=args,\n",
    "                rand_set_all=rand_set_all,)\n",
    "main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e39c17-60dd-4f56-b22f-a938e598eaa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Simulate the args like in the `main_*.py` files\n",
    "class ARGS:\n",
    "    #federated arguments\n",
    "    # epochs:int = 1000         # rounds of training\n",
    "    epochs:int = 10           # rounds of training\n",
    "    num_users:int = 100       # number of users: K\n",
    "    shard_per_user:int = 2    # classes per user\n",
    "    frac:float = 0.1          # the fraction of clients: C\n",
    "    local_ep:int = 1          # the number of local epochs: E\n",
    "    local_bs:int = 10         # local batch size: B\n",
    "    bs:int = 128              # test batch size\n",
    "    lr:float = 0.01           # learning rate\n",
    "    # results_save:str = \"run1\"\n",
    "    momentum:float = 0.5      # SGD momentum (default: 0.5)\n",
    "    # gpu:int = 0\n",
    "    split:str = \"user\"        # train-test split type, user or sample\n",
    "    # grad_norm:str           # use_gradnorm_avging\n",
    "    local_ep_pretrain:int = 0 # the number of pretrain local ep\n",
    "    lr_decay:float = 1.0      # learning rate decay per round\n",
    "\n",
    "    # model arguments\n",
    "    model:str = \"cnn\"          # model name\n",
    "    kernel_num:int = 9         # number of each kind of kernel\n",
    "    kernel_sizes:str = \"3,4,5\" # comma-separated kernel size to use for convolution\n",
    "    norm:str = \"batch_norm\"    # batch_norm, layer_norm, or None\n",
    "    num_filters:int = 32       # number of filters for conv nets\n",
    "    max_pool:str = True        # whether use max pooling rather than strided convolutions\n",
    "    num_layers_keep:int = 1    # number layers to keep\n",
    "    \n",
    "    # other arguments\n",
    "    dataset:str = \"coba\"      # name of dataset\n",
    "    log_level:str = \"info\"    # level of logger\n",
    "    iid:bool = True           # \"store_true\" #whether iid or not\n",
    "    num_classes:int = 14      # number of classes\n",
    "    num_channels:int = 3      # number of channels of images RGB\n",
    "    gpu:int = 0               # GPU ID, -1 for CPU\n",
    "    stopping_rounds:int = 10  # rounds of early stopping\n",
    "    verbose:bool = True       # \"store_true\"\n",
    "    print_freq:int = 100      # print loss frequency during training\n",
    "    seed:int = 1              # random seed (default:1)\n",
    "    test_freq:int = 1         # how often to test on val set\n",
    "    load_fed:str = \"\"         # define pretrained federated model path\n",
    "    results_save:str = \"run1\" # define fed results save folder\n",
    "    start_saving:int = 0      # when to start saving models\n",
    "\n",
    "\n",
    "args = ARGS()\n",
    "\n",
    "args.device = torch.device(\n",
    "        \"cuda:{}\".format(args.gpu)\n",
    "        if torch.cuda.is_available() and args.gpu != -1\n",
    "        else \"cpu\"\n",
    ")\n",
    "\n",
    "args.num_users, args.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e1563-27a4-4b31-bdb6-83ce780dc413",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615bbab2-f12d-4e6e-9581-82de8b2d74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# args.device = torch.device(\n",
    "#         \"cuda:{}\".format(args.gpu)\n",
    "#         if torch.cuda.is_available() and args.gpu != -1\n",
    "#         else \"cpu\"\n",
    "# )\n",
    "\n",
    "# dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "# if args.dataset == \"coba\":\n",
    "#     dataset_train, dataset_test = dataset_train.dataset, dataset_test.dataset\n",
    "\n",
    "# model = get_model(args)\n",
    "\n",
    "# model_state_dict_path:str = Path(\"save\",\"coba\",\"cnn_iidFalse_num98_C0.3_le1\",\"shard2\",\"seed10_coba_fedavg_bestcase_run12\",\"fed\",\"model_1000.pt\")\n",
    "\n",
    "# model.load_state_dict(torch.load(model_state_dict_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc13d5-0c4b-49b2-b101-9c8ad9eaac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader: DataLoader = DataLoader(dataset_test, batch_size=args.bs)\n",
    "# IS_USING_GPU: bool = args.gpu != -1 and args.device.type != \"cpu\"\n",
    "\n",
    "# probs: np.array = np.array([])\n",
    "# y_preds: np.array = np.array([])\n",
    "# y_trues: np.array = np.array([])\n",
    "# coba_cms = []\n",
    "\n",
    "# for _, (data, target) in enumerate(data_loader):\n",
    "#     if args.gpu != -1 and args.device.type != \"cpu\":\n",
    "#         data, target = data.to(args.device), target.to(args.device)\n",
    "#     if args.dataset == \"coba\":\n",
    "#         data = data.permute(0, 3, 1, 2)\n",
    "\n",
    "#     log_probs: torch.Tensor = model(data)\n",
    "\n",
    "#     probs: np.array = (\n",
    "#             np.append(probs, log_probs.cpu().data.numpy())\n",
    "#             if IS_USING_GPU\n",
    "#             else np.append(probs, log_probs.data.numpy())\n",
    "#     )\n",
    "\n",
    "#     y_pred: torch.Tensor = (\n",
    "#             log_probs.cpu().data.max(1, keepdim=True)[1]\n",
    "#             if args.device.type != \"cpu\"\n",
    "#             else log_probs.data.max(1, keepdim=True)[1]\n",
    "#     )\n",
    "\n",
    "#     y_true: torch.Tensor = (\n",
    "#             torch.tensor(\n",
    "#                 list(map(torch.argmax, target.data)), device=\"cpu\"\n",
    "#             ).data.view_as(y_pred)\n",
    "#             if args.dataset == \"coba\"\n",
    "#             else target.to(\"cpu\").data.view_as(y_pred)\n",
    "#     )\n",
    "\n",
    "#     coba_cms.append(confusion_matrix(y_pred=y_pred,y_true=y_true))\n",
    "\n",
    "#     y_preds = np.append(y_preds,y_pred)\n",
    "#     y_trues = np.append(y_trues,y_true)\n",
    "\n",
    "# y_preds.shape == y_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cc9ea-d23c-4700-9843-9a91812e34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba_cm = confusion_matrix(y_pred=y_preds,y_true=y_trues)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=coba_cm)\n",
    "# disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b4658-53dd-4013-99cb-0d709174f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba_cm_gen = (cm for cm in coba_cms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961ee1d-0296-4e07-8011-647aebc8ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disp = ConfusionMatrixDisplay(confusion_matrix=next(coba_cm_gen))\n",
    "# disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a758ae-61d2-44e2-ac0b-4454a62e7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(coba_cms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b0a31-9f36-45f1-924d-92a362fac4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, target in data_loader:\n",
    "#     print(data.shape)\n",
    "#     print(target.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89798028-4a70-4fcf-8258-460350d31737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_test == dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebee746-e0ca-44c5-844d-20d7102c9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba_dataset: COBA = COBA(root=\"data/coba\", download=True)\n",
    "\n",
    "# ## Create training and testing data\n",
    "# train_size: int = int(\n",
    "#     0.8 * len(coba_dataset)\n",
    "# )  # maybe TODO: make the percentage customizable (part of `args`)\n",
    "# test_size: int = len(coba_dataset) - train_size\n",
    "# dataset_train, dataset_test = random_split(\n",
    "#     dataset=coba_dataset, lengths=[train_size, test_size]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43665c55-63d5-4ec5-93a1-19d19680367c",
   "metadata": {},
   "source": [
    "## Verify Train/Test data differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc09ae-9bb4-4923-b393-4a1870db1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_dataset: COBA = COBA(root=\"data/coba\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13bc82-4b09-4bbe-b53a-56de88ae1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size: int = int(\n",
    "            0.8 * len(coba_dataset)\n",
    ")  # maybe TODO: make the percentage customizable (part of `args`)\n",
    "test_size: int = len(coba_dataset) - train_size\n",
    "dataset_train, dataset_test = random_split(\n",
    "    dataset=coba_dataset, lengths=[train_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1099d-cc17-40ba-a310-43cb3ce7ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf8ad2-1b1f-48f0-b17e-42e07d6f33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_train = [(img,label) for img,label in dataset_train]\n",
    "len(new_dataset_train) == train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7401b2d-0f7a-47b5-8b4e-63a649b9b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.coba_dataset import COBA_Split\n",
    "d_train = COBA_Split(dataset=dataset_train)\n",
    "d_test = COBA_Split(dataset=dataset_test)\n",
    "\n",
    "len(d_train), len(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1201e5-1867-4140-9618-7f4b37335e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid\n",
    "dict_users_train: Dict[int, set] = iid(\n",
    "                dataset=d_train, args=args\n",
    ")\n",
    "dict_users_test: Dict[int, set] = iid(\n",
    "    dataset=d_test, args=args\n",
    ")\n",
    "for user,d in dict_users_train.items():\n",
    "    print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc61a0-bcbe-4301-941d-d17e007df7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noniid\n",
    "dict_users_train, rand_set_all = noniid(\n",
    "                dataset=d_train, args=args\n",
    ")\n",
    "dict_users_test, rand_set_all = noniid(\n",
    "    dataset=d_test,\n",
    "    args=args,\n",
    "    rand_set_all=rand_set_all,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ea8c2-a8e6-4d75-a126-f69b70dcb06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for user,d in dict_users_train.items():\n",
    "    print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76ff6d-9bd2-4bac-af56-41da4b5fa032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
