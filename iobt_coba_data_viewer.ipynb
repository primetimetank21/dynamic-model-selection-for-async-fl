{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27634864-d12d-435e-8d4d-f32844ece41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "# import os\n",
    "from pathlib import Path\n",
    "# import shutil\n",
    "import warnings\n",
    "import opendatasets as od\n",
    "from typing import Optional, Callable, Tuple, Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# import torchvision\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets.vision import VisionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9696563-e695-4bb6-afa0-97f634d23856",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate the args like in the `main_*.py` files\n",
    "class ARGS:\n",
    "    num_users:int = 100\n",
    "\n",
    "args = ARGS()\n",
    "args.num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507edab-0ccb-4744-bb21-d990155ab775",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define custom CobaDataset class\n",
    "class COBA(VisionDataset):\n",
    "    \"\"\"\n",
    "    `COBA <https://www.kaggle.com/datasets/earltankardjr/coba-iobt-dataset> Dataset`\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``COBA/raw/data.pt``\n",
    "            and  ``COBA/raw/targets.pt`` exist.\n",
    "        train (bool, optional): If `True`, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``. -- This currently doesn't do anything!\n",
    "        download (bool, optional): If `True`, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "    resources:list = [\"https://www.kaggle.com/datasets/earltankardjr/coba-iobt-dataset\"] # link(s) to coba dataset\n",
    "    training_file:str = \"training.pt\"\n",
    "    test_file:str = \"test.pt\"\n",
    "    # training_file:str = \"training.pt\"\n",
    "    # test_file:str = \"test.pt\"\n",
    "    classes:list = [\"0 - airplane\",\n",
    "                    \"1 - ambulance\",\n",
    "                    \"2 - briefcase\",\n",
    "                    \"3 - cannon\",\n",
    "                    \"4 - car\",\n",
    "                    \"5 - civilian\",\n",
    "                    \"6 - dagger\",\n",
    "                    \"7 - dog\",\n",
    "                    \"8 - handgun\",\n",
    "                    \"9 - missilerocket\",\n",
    "                    \"10 - rifle\",\n",
    "                    \"11 - soldier\",\n",
    "                    \"12 - tank\",\n",
    "                    \"13 - truck\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def train_labels(self) -> torch.Tensor:\n",
    "        warnings.warn(\"train_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def test_labels(self) -> torch.Tensor:\n",
    "        warnings.warn(\"test_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def train_data(self) -> torch.Tensor:\n",
    "        warnings.warn(\"train_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    @property\n",
    "    def test_data(self) -> torch.Tensor:\n",
    "        warnings.warn(\"test_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    def __init__(self, root:str, train:bool=True, transform:Optional[Callable]=None, target_transform: Optional[Callable]=None, download:bool=False) -> None:\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "        self.train = train\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train:\n",
    "            data_file = self.training_file\n",
    "        else:\n",
    "            data_file = self.test_file\n",
    "        \n",
    "        self.data, self.targets = self._load_data()\n",
    "\n",
    "\n",
    "    def download(self) -> None:\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # Make Raw directory\n",
    "        Path.mkdir(self.raw_folder, exist_ok=True, parents=True)\n",
    "\n",
    "        # Download dataset file\n",
    "        coba_dataset_url:str = self.resources[0]\n",
    "        od.download(coba_dataset_url)\n",
    "\n",
    "        ## Move dataset\n",
    "        coba_iobt_npy_file:str = \"iobt_128_128.npy\"\n",
    "\n",
    "        downloaded_coba_dataset_path = Path.cwd().joinpath(\"coba-iobt-dataset\", coba_iobt_npy_file)\n",
    "        \n",
    "        if not downloaded_coba_dataset_path.exists():\n",
    "            raise Exception(\"Failed to download and locate COBA dataset\")\n",
    "        \n",
    "        new_coba_dataset_path = Path.cwd().joinpath(self.raw_folder, coba_iobt_npy_file)\n",
    "        downloaded_coba_dataset_path.rename(new_coba_dataset_path)\n",
    "        \n",
    "        ## Format dataset\n",
    "        with open(new_coba_dataset_path, \"rb\") as f:\n",
    "            data = torch.tensor(np.load(f), dtype=torch.float32)\n",
    "            targets = torch.tensor(np.load(f, allow_pickle=True), dtype=torch.int64)\n",
    "\n",
    "        ## Save dataset\n",
    "        torch.save(data, self.raw_folder.joinpath(\"data.pt\"))\n",
    "        torch.save(targets, self.raw_folder.joinpath(\"targets.pt\"))\n",
    "        \n",
    "        ## Delete unnecessary files/directories\n",
    "        Path.rmdir(Path.cwd().joinpath(\"coba-iobt-dataset\"))\n",
    "        Path.unlink(Path.cwd().joinpath(self.raw_folder, coba_iobt_npy_file))\n",
    "\n",
    "    def _load_data(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image_file:str = \"data.pt\"\n",
    "        data = torch.load(Path.cwd().joinpath(self.raw_folder, image_file))\n",
    "        \n",
    "        label_file:str = \"targets.pt\"\n",
    "        targets = torch.load(Path.cwd().joinpath(self.raw_folder, label_file))\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "        # return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        # label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self) -> Path:\n",
    "        return Path(self.root, self.__class__.__name__, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def processed_folder(self) -> Path:\n",
    "        return Path(self.root, self.__class__.__name__, \"processed\")\n",
    "\n",
    "    @property\n",
    "    def class_to_idx(self) -> Dict[int, str]:\n",
    "        # class_dict:dict[int,str] = {int(label.replace(\" \",\"\").split(\"-\")[0]): label.replace(\" \",\"\").split(\"-\")[1] for label in self.classes} #one liner\n",
    "        class_dict:Dict[int,str] = {}\n",
    "        for label in self.classes:\n",
    "            encoded_val, name = label.replace(\" \",\"\").split(\"-\")\n",
    "            class_dict[int(encoded_val)] = name\n",
    "        return class_dict\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        return Path.exists(Path(self.raw_folder, \"data.pt\")) and Path.exists(Path(self.raw_folder, \"targets.pt\"))\n",
    "        # return (Path.exists(Path.joinpath(self.raw_folder, self.training_file)) and Path.exists(Path.joinpath(self.raw_folder, self.test_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4573af6-6dd9-4dc6-a582-4324b9e981dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize CobaDataset\n",
    "coba_dataset = COBA(root=\"data/coba\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83259c24-1f2b-4403-94b0-6e119d59e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training and testing data -- method 1\n",
    "train_size = int(0.8 * len(coba_dataset))\n",
    "test_size = len(coba_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset=coba_dataset, lengths=[train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea2319-70bc-463d-ac2f-67b5c93b2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08560e48-1eb9-40a4-99b8-4fa05b5a5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot random train sample\n",
    "example_image = coba_dataset[np.random.choice(train_dataset.indices, 1).item()]\n",
    "img, label = example_image\n",
    "label_encodings = train_dataset.dataset.class_to_idx\n",
    "\n",
    "print(f\"Label: {label_encodings[label.item()]}\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c41e0f-be5e-40d9-a47e-6048011c7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The random split works!\n",
    "dups = 0\n",
    "for index in test_dataset.indices:\n",
    "    if index in train_dataset.indices:\n",
    "        dups += 1\n",
    "print(dups)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037c085-5029-4b24-9687-0918f9ec76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try iid example\n",
    "def iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d612673-5e15-47b1-ada5-f866fa077238",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users_train = iid(dataset=train_dataset.dataset, num_users=100)\n",
    "for user,d in dict_users_train.items():\n",
    "    print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b7d7d-b0f9-4878-9763-aa35c2390c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try noniid example\n",
    "import random\n",
    "\n",
    "def noniid(dataset, num_users, shard_per_user, rand_set_all=[]):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "\n",
    "    idxs_dict = {}\n",
    "    for i in range(len(dataset)):\n",
    "        # label = torch.tensor(dataset.targets[i]).item()\n",
    "        label = dataset.targets[i].item()\n",
    "        if label not in idxs_dict.keys():\n",
    "            idxs_dict[label] = []\n",
    "        idxs_dict[label].append(i)\n",
    "\n",
    "    num_classes = len(np.unique(dataset.targets))\n",
    "    shard_per_class = int(shard_per_user * num_users / num_classes)\n",
    "\n",
    "    #debug\n",
    "    print(f\"num_classes: {num_classes}\")\n",
    "    print(f\"shard_per_class: {shard_per_class}\")\n",
    "    \n",
    "    for label in idxs_dict.keys():\n",
    "        x = idxs_dict[label]\n",
    "        num_leftover = len(x) % shard_per_class\n",
    "        leftover = x[-num_leftover:] if num_leftover > 0 else []\n",
    "        x = np.array(x[:-num_leftover]) if num_leftover > 0 else np.array(x)\n",
    "        x = x.reshape((shard_per_class, -1))\n",
    "        x = list(x)\n",
    "\n",
    "        #debug\n",
    "        # print(f\"x: {x}\")\n",
    "\n",
    "        for i, idx in enumerate(leftover):\n",
    "            x[i] = np.concatenate([x[i], [idx]])\n",
    "        idxs_dict[label] = x\n",
    "\n",
    "        #debug\n",
    "        # print(f\"idxs_dict: {idxs_dict}\")\n",
    "\n",
    "    if len(rand_set_all) == 0:\n",
    "        rand_set_all = list(range(num_classes)) * shard_per_class\n",
    "        random.shuffle(rand_set_all)\n",
    "        \n",
    "        #debug\n",
    "        print(f\"len(rand_set_all) = {len(rand_set_all)}\")\n",
    "\n",
    "        try:\n",
    "            rand_set_all = np.array(rand_set_all).reshape((num_users, -1))\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError: {ve}\\n\\nAttempting to reshape...\")\n",
    "            for i in range(num_users,0,-1):\n",
    "                try:\n",
    "                    rand_set_all = np.array(rand_set_all).reshape((i, -1))\n",
    "                    print(f\"New num_users: {i}\")\n",
    "                    global args\n",
    "                    args.num_users = i\n",
    "                    num_users = i\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        print(f\"rand_set_all.shape: {rand_set_all.shape}\")\n",
    "    \n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set_label = rand_set_all[i]\n",
    "        rand_set = []\n",
    "        for label in rand_set_label:\n",
    "            idx = np.random.choice(len(idxs_dict[label]), replace=False)\n",
    "            rand_set.append(idxs_dict[label].pop(idx))\n",
    "        dict_users[i] = np.concatenate(rand_set)\n",
    "\n",
    "    dict_users = {key: val for key,val in dict_users.items() if len(val)}\n",
    "    \n",
    "    test = []\n",
    "    for key, value in dict_users.items():\n",
    "        # x = np.unique(torch.tensor(dataset.targets)[value])\n",
    "        x = np.unique(dataset.targets[value])\n",
    "        assert(len(x)) <= shard_per_user\n",
    "        test.append(value)\n",
    "    test = np.concatenate(test)\n",
    "    assert(len(test) == len(dataset))\n",
    "    assert(len(set(list(test))) == len(dataset))\n",
    "\n",
    "    return dict_users, rand_set_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12220da-6b33-491f-bf5a-996eec479d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users_train, rand_set_all = noniid(dataset=train_dataset.dataset, num_users=100, shard_per_user=2)\n",
    "for user,d in dict_users_train.items():\n",
    "    print(f\"user:{user}\\t\\t len:{len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e640a-ac77-46f4-ace4-9699abafa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa317f4-185d-40a6-955f-582fa30df154",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04262752-e7ed-4c48-a7e9-5d598033c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from utils.options import args_parser\n",
    "from utils.train_utils import get_data, get_model\n",
    "from models.Update import LocalUpdate\n",
    "from models.test import test_img\n",
    "import os\n",
    "## Simulate the args like in the `main_*.py` files\n",
    "class ARGS:\n",
    "    #federated arguments\n",
    "    epochs:int = 1000         # rounds of training\n",
    "    num_users:int = 100       # number of users: K\n",
    "    shard_per_user:int = 2    # classes per user\n",
    "    frac:float = 0.1          # the fraction of clients: C\n",
    "    local_ep:int = 1          # the number of local epochs: E\n",
    "    local_bs:int = 10         # local batch size: B\n",
    "    bs:int = 128              # test batch size\n",
    "    lr:float = 0.01           # learning rate\n",
    "    # results_save:str = \"run1\"\n",
    "    momentum:float = 0.5      # SGD momentum (default: 0.5)\n",
    "    # gpu:int = 0\n",
    "    split:str = \"user\"        # train-test split type, user or sample\n",
    "    # grad_norm:str           # use_gradnorm_avging\n",
    "    local_ep_pretrain:int = 0 # the number of pretrain local ep\n",
    "    lr_decay:float = 1.0      # learning rate decay per round\n",
    "\n",
    "    # model arguments\n",
    "    model:str = \"cnn\"          # model name\n",
    "    kernel_num:int = 9         # number of each kind of kernel\n",
    "    kernel_sizes:str = \"3,4,5\" # comma-separated kernel size to use for convolution\n",
    "    norm:str = \"batch_norm\"    # batch_norm, layer_norm, or None\n",
    "    num_filters:int = 32       # number of filters for conv nets\n",
    "    max_pool:str = True        # whether use max pooling rather than strided convolutions\n",
    "    num_layers_keep:int = 1    # number layers to keep\n",
    "    \n",
    "    # other arguments\n",
    "    dataset:str = \"coba\"     # name of dataset\n",
    "    iid:bool = True          # \"store_true\" #whether iid or not\n",
    "    num_classes:int = 14     # number of classes\n",
    "    num_channels:int = 3     # number of channels of images RGB\n",
    "    gpu:int = 0              # GPU ID, -1 for CPU\n",
    "    stopping_rounds:int = 10 # rounds of early stopping\n",
    "    verbose:bool = True      # \"store_true\"\n",
    "    print_freq:int = 100     # print loss frequency during training\n",
    "    seed:int = 1             # random seed (default:1)\n",
    "    test_freq:int = 1        # how often to test on val set\n",
    "    load_fed:str = \"\"        # define pretrained federated model path\n",
    "    results_save:str = \"/\"   # define fed results save folder\n",
    "    start_saving:int = 0     # when to start saving models\n",
    "\n",
    "\n",
    "args = ARGS()\n",
    "args.num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6ee77-fa9e-4092-b219-6e3dd062c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\n",
    "        \"cuda:{}\".format(args.gpu)\n",
    "        if torch.cuda.is_available() and args.gpu != -1\n",
    "        else \"cpu\"\n",
    ")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ed093-9054-4fb0-b37f-dee0223faf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "\n",
    "if args.dataset == \"coba\":\n",
    "    dataset_train, dataset_test = dataset_train.dataset, dataset_test.dataset\n",
    "\n",
    "base_dir = \"./save/{}/{}_iid{}_num{}_C{}_le{}/shard{}/{}/\".format(\n",
    "    args.dataset,\n",
    "    args.model,\n",
    "    args.iid,\n",
    "    args.num_users,\n",
    "    args.frac,\n",
    "    args.local_ep,\n",
    "    args.shard_per_user,\n",
    "    args.results_save,\n",
    ")\n",
    "if not os.path.exists(os.path.join(base_dir, \"fed\")):\n",
    "    os.makedirs(os.path.join(base_dir, \"fed\"), exist_ok=True)\n",
    "\n",
    "dict_save_path = os.path.join(base_dir, \"dict_users.pkl\")\n",
    "with open(dict_save_path, \"wb\") as handle:\n",
    "    pickle.dump((dict_users_train, dict_users_test), handle)\n",
    "\n",
    "# build model\n",
    "net_glob = get_model(args)\n",
    "net_glob.train()\n",
    "\n",
    "# training\n",
    "results_save_path = os.path.join(base_dir, \"fed/results.csv\")\n",
    "\n",
    "loss_train = []\n",
    "net_best = None\n",
    "best_loss = None\n",
    "best_acc = None\n",
    "best_epoch = None\n",
    "\n",
    "lr = args.lr\n",
    "results = []\n",
    "\n",
    "for _iter in range(args.epochs):\n",
    "    w_glob = None\n",
    "    loss_locals = []\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    print(\"Round {}, lr: {:.6f}, {}\".format(_iter, lr, idxs_users))\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(\n",
    "            args=args, dataset=dataset_train, idxs=dict_users_train[idx]\n",
    "        )\n",
    "        net_local = copy.deepcopy(net_glob)\n",
    "\n",
    "        w_local, loss = local.train(net=net_local.to(args.device))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "        if w_glob is None:\n",
    "            w_glob = copy.deepcopy(w_local)\n",
    "        else:\n",
    "            for k in w_glob.keys():\n",
    "                w_glob[k] += w_local[k]\n",
    "\n",
    "    lr *= args.lr_decay\n",
    "\n",
    "    # update global weights\n",
    "    for k in w_glob.keys():\n",
    "        w_glob[k] = torch.div(w_glob[k], m)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    loss_train.append(loss_avg)\n",
    "\n",
    "    if (_iter + 1) % args.test_freq == 0:\n",
    "        net_glob.eval()\n",
    "\n",
    "        # pylint: disable=unbalanced-tuple-unpacking\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        print(\n",
    "            \"Round {:3d}, Average loss {:.3f}, Test loss {:.3f}, Test accuracy: {:.2f}\".format(\n",
    "                _iter, loss_avg, loss_test, acc_test\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if best_acc is None or acc_test > best_acc:\n",
    "            net_best = copy.deepcopy(net_glob)\n",
    "            best_acc = acc_test\n",
    "            best_epoch = _iter\n",
    "\n",
    "        # if (iter + 1) > args.start_saving:\n",
    "        #     model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(_iter + 1))\n",
    "        #     torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "        results.append(np.array([_iter, loss_avg, loss_test, acc_test, best_acc]))\n",
    "        final_results = np.array(results)\n",
    "        final_results = pd.DataFrame(\n",
    "            final_results,\n",
    "            columns=[\"epoch\", \"loss_avg\", \"loss_test\", \"acc_test\", \"best_acc\"],\n",
    "        )\n",
    "        final_results.to_csv(results_save_path, index=False)\n",
    "\n",
    "    if (_iter + 1) % 50 == 0:\n",
    "        best_save_path = os.path.join(base_dir, \"fed/best_{}.pt\".format(_iter + 1))\n",
    "        model_save_path = os.path.join(\n",
    "            base_dir, \"fed/model_{}.pt\".format(_iter + 1)\n",
    "        )\n",
    "        torch.save(net_best.state_dict(), best_save_path)\n",
    "        torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "print(\"Best model, iter: {}, acc: {}\".format(best_epoch, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Troubleshooting with Shape errors\n",
    "# image,label = coba_dataset[0]\n",
    "# print(image.shape)\n",
    "# image = image.permute(2,0,1)\n",
    "# print(image.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
