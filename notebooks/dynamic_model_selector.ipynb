{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c17936-35dc-4d71-a35f-c350e69d4f16",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5ad90-4814-46fe-bd51-44a3783ce348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Union\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.train_utils import get_model\n",
    "from utils.coba_dataset import COBA\n",
    "\n",
    "from models.test import test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a1400-71b5-4daf-845c-92e92b113e0e",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce713305-42f6-4642-8766-a78605ba4e98",
   "metadata": {},
   "source": [
    "1. Get paths of models (given a directory)\n",
    "1. Store paths of models\n",
    "1. For each model...\n",
    "   1. Load into memory\n",
    "   2. Test model\n",
    "   3. Compare to the current best models for each performance metric, replacing if necessary\n",
    "1. Save best models to `best_models.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afaa7ca-6c3a-4399-abb1-28fdb3b76018",
   "metadata": {},
   "source": [
    "# Dynamically Selecting the Best Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d633b-6096-4b58-9115-06330feb2cfb",
   "metadata": {},
   "source": [
    "## 1. Get paths of models (given a directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9bae0-962f-4ac2-aa74-7b1137b68174",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate the args like in the `main_*.py` files\n",
    "class ARGS:\n",
    "    # federated arguments\n",
    "    # epochs:int = 1000         # rounds of training\n",
    "    epochs: int = 10  # rounds of training\n",
    "    train_test_same: int = 0  # use same testing for\n",
    "    num_users: int = 100  # number of users: K\n",
    "    shard_per_user: int = 2  # classes per user\n",
    "    frac: float = 0.1  # the fraction of clients: C\n",
    "    local_ep: int = 1  # the number of local epochs: E\n",
    "    local_bs: int = 10  # local batch size: B\n",
    "    bs: int = 128  # test batch size\n",
    "    lr: float = 0.01  # learning rate\n",
    "    # results_save:str = \"run1\"\n",
    "    momentum: float = 0.5  # SGD momentum (default: 0.5)\n",
    "    # gpu:int = 0\n",
    "    split: str = \"user\"  # train-test split type, user or sample\n",
    "    # grad_norm:str           # use_gradnorm_avging\n",
    "    local_ep_pretrain: int = 0  # the number of pretrain local ep\n",
    "    lr_decay: float = 1.0  # learning rate decay per round\n",
    "\n",
    "    # model arguments\n",
    "    model: str = \"cnn\"  # model name\n",
    "    kernel_num: int = 9  # number of each kind of kernel\n",
    "    kernel_sizes: str = \"3,4,5\"  # comma-separated kernel size to use for convolution\n",
    "    norm: str = \"batch_norm\"  # batch_norm, layer_norm, or None\n",
    "    num_filters: int = 32  # number of filters for conv nets\n",
    "    max_pool: str = True  # whether use max pooling rather than strided convolutions\n",
    "    num_layers_keep: int = 1  # number layers to keep\n",
    "\n",
    "    # other arguments\n",
    "    dataset: str = \"coba\"  # name of dataset\n",
    "    log_level: str = \"warning\"  # level of logger\n",
    "    iid: bool = True  # \"store_true\" #whether iid or not\n",
    "    num_classes: int = 14  # number of classes\n",
    "    num_channels: int = 3  # number of channels of images RGB\n",
    "    gpu: int = 0  # GPU ID, -1 for CPU\n",
    "    stopping_rounds: int = 10  # rounds of early stopping\n",
    "    verbose: bool = True  # \"store_true\"\n",
    "    print_freq: int = 100  # print loss frequency during training\n",
    "    seed: int = 1  # random seed (default:1)\n",
    "    test_freq: int = 1  # how often to test on val set\n",
    "    load_fed: str = \"\"  # define pretrained federated model path\n",
    "    results_save: str = \"run1\"  # define fed results save folder\n",
    "    start_saving: int = 0  # when to start saving models\n",
    "\n",
    "\n",
    "args = ARGS()\n",
    "\n",
    "args.device = torch.device(\n",
    "    \"cuda:{}\".format(args.gpu)\n",
    "    if torch.cuda.is_available() and args.gpu != -1\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "args.num_users, args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd6a91-e68d-499c-9218-4632761810ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import dynamic_model_selector_and_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65068c17-891f-408d-ac50-c2bfd4425259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args.num_users = 98\n",
    "args.frac = \"0.3\"\n",
    "args.iid = False\n",
    "args.log_level = \"info\"  # \"debug\" might crash the notebook\n",
    "args.seed = 0\n",
    "args.results_save = \"coba_fedavg_bestcase_run2\"\n",
    "\n",
    "base_dir: Path = Path(\n",
    "    Path.cwd().parent,\n",
    "    \"save\",\n",
    "    \"coba_legacy\",\n",
    "    f\"{args.model}_iid{args.iid}_num{args.num_users}_C{args.frac}_le{args.local_ep}\",\n",
    "    f\"shard{args.shard_per_user}\",\n",
    ")\n",
    "\n",
    "base_dir = Path(base_dir, f\"seed{args.seed}_{args.results_save}\")\n",
    "\n",
    "dynamic_model_selector_and_saver(args=args, base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc8302-d1bf-4742-a56c-f712bbd52b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCENARIO: str = \"best\"  # the scenario we're interested in\n",
    "# SEED: int = 0  # the seed of the experiment we're interested in analyzing\n",
    "# chosen_scenario_dir: Optional[Path] = None\n",
    "# experiment_run_dir: Path = Path(\n",
    "#     Path.cwd().parent, \"save\", \"coba_legacy\"\n",
    "# )  # could also be: \"coba\", \"mnist\", \"cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9436e9-122c-4be3-98f2-ecbba1ea97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get models according to the chosen scenario\n",
    "# async_fl_scenarios: Dict[str, str] = {\"0.3\": \"best\", \"0.5\": \"average\", \"1.0\": \"worst\"}\n",
    "\n",
    "# print(\"Async FL Scenarios:\")\n",
    "# for dir in experiment_run_dir.glob(\"*\"):\n",
    "#     scenario_percent: str = (\n",
    "#         dir.as_posix().split(os.sep)[-1].split(\"_\")[-2].replace(\"C\", \"\")\n",
    "#     )\n",
    "#     print(f\"\\t{scenario_percent} -> {async_fl_scenarios[scenario_percent]}\")\n",
    "\n",
    "#     if async_fl_scenarios[scenario_percent] == SCENARIO:\n",
    "#         chosen_scenario_dir = dir\n",
    "\n",
    "# print(f\"{SCENARIO.title()} case scenario directory: '{chosen_scenario_dir.as_posix()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636294ce-7210-439d-9046-d207bb498b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get paths to models in the chosen scenario\n",
    "# all_chosen_scenario_runs_dir: Path = Path(chosen_scenario_dir, \"shard2\")\n",
    "# all_models_dir: Optional[Path] = None\n",
    "\n",
    "# for dir in all_chosen_scenario_runs_dir.glob(\"*\"):\n",
    "#     seed: int = int(dir.as_posix().split(os.sep)[-1].split(\"_\")[0].split(\"d\")[-1])\n",
    "\n",
    "#     if SEED == seed:\n",
    "#         all_models_dir = Path(dir, \"fed\")\n",
    "\n",
    "# # Raise error if no directory is found with the given SEED\n",
    "# if all_models_dir is None:\n",
    "#     raise FileNotFoundError(\n",
    "#         f\"Directory with the provided seed '{SEED}' does not exist. Please choose a different one and try again.\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# print(\n",
    "#     [\n",
    "#         model_file.as_posix().split(os.sep)[-1]\n",
    "#         for model_file in all_models_dir.glob(\"*.pt\")\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f12f7d-041a-452f-814c-f77c4cf4b174",
   "metadata": {},
   "source": [
    "## 2. Store paths of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f441c9-f502-43b7-b59e-deb8909050ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_paths:List[Path] = sorted([model_file for model_file in all_models_dir.glob(\"*.pt\")],key=lambda s: int(s.as_posix().split(os.sep)[-1].split(\"_\")[-1].replace(\".pt\",\"\")))\n",
    "# model_paths: List[Path] = [model_file for model_file in all_models_dir.glob(\"*.pt\")]\n",
    "\n",
    "# REMOVE_BEST: bool = True  # This is to remove the presumed \"best\" model files\n",
    "# if REMOVE_BEST:\n",
    "#     model_paths = np.array(model_paths)[\n",
    "#         list(\"best_\" not in model_path.as_posix() for model_path in model_paths)\n",
    "#     ].tolist()\n",
    "\n",
    "# model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde7041-43b2-423a-82a5-0baa7f69352f",
   "metadata": {},
   "source": [
    "## 3. For each model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa2281",
   "metadata": {},
   "source": [
    "1. Load into memory\n",
    "2. Test model\n",
    "3. Compare to the current best models for each performance metric, replacing if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539346c-901b-48ad-8a8f-419a0d60ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ChosenModel:\n",
    "#     def __init__(\n",
    "#         self, path: Path, performance_metrics: Dict[str, float], main_metric: str\n",
    "#     ) -> None:\n",
    "#         self.path: Path = path\n",
    "#         self.performance_metrics: Dict[str, float] = performance_metrics\n",
    "#         self.main_metric: str = main_metric\n",
    "#         self.main_metric_value: float = performance_metrics[main_metric]\n",
    "\n",
    "\n",
    "# def update_metric_value(old_val: float, new_val: float, metric: str) -> bool:\n",
    "#     return new_val < old_val if metric == \"loss\" else new_val > old_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182daaac-72ec-4bec-9d0d-8a6a881866af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics: List[str] = [\"accuracy\", \"loss\", \"f1\", \"precision\", \"recall\"]\n",
    "# best_models: Dict[str, Optional[ChosenModel]] = {metric: None for metric in metrics}\n",
    "\n",
    "# coba_dataset: COBA = COBA(root=\"../data/coba\", download=True)\n",
    "\n",
    "# ## Create training and testing data -- method 1\n",
    "# train_size = int(0.8 * len(coba_dataset))\n",
    "# test_size = len(coba_dataset) - train_size\n",
    "# _, test_dataset = random_split(dataset=coba_dataset, lengths=[train_size, test_size])\n",
    "\n",
    "# test_dataset = test_dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa3f9a-283e-4156-98f8-afc248e483a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for model_path in model_paths:\n",
    "#     # Load into memory\n",
    "#     model = get_model(args)\n",
    "\n",
    "#     model.load_state_dict(\n",
    "#         torch.load(model_path)\n",
    "#     ) if args.device.type != \"cpu\" else model.load_state_dict(\n",
    "#         torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "#     )\n",
    "\n",
    "#     # Test model\n",
    "#     acc_test, loss_test, f1_test, precision_test, recall_test = test_img(\n",
    "#         model, test_dataset, args\n",
    "#     )\n",
    "#     results: Dict[str, float] = {\n",
    "#         metric: value\n",
    "#         for metric, value in zip(\n",
    "#             metrics, [acc_test, loss_test, f1_test, precision_test, recall_test]\n",
    "#         )\n",
    "#     }\n",
    "\n",
    "#     # Compare to the current best models for each performance metric, replacing if necessary\n",
    "#     for metric, metric_value in results.items():\n",
    "#         if best_models[metric] is None or update_metric_value(\n",
    "#             old_val=best_models[metric].main_metric_value,\n",
    "#             new_val=metric_value,\n",
    "#             metric=metric,\n",
    "#         ):\n",
    "#             best_models[metric]: ChosenModel = ChosenModel(\n",
    "#                 path=model_path, performance_metrics=results, main_metric=metric\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65438e0-506d-4802-ba8b-dba86dff8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, model in best_models.items():\n",
    "#     print(f\"Key: {key.capitalize()}\")\n",
    "#     print(f\"\\tBest Model Filename: {model.path.as_posix().split(os.sep)[-1]}\")\n",
    "#     print(f\"\\tBest Model Main Metric Value: {model.main_metric_value}\")\n",
    "#     print(f\"\\tBest Model All Metric Values:\")\n",
    "#     for metric, value in model.performance_metrics.items():\n",
    "#         print(f\"\\t\\t{metric.capitalize():9}:\\t{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b138b-fd0e-413a-a742-cf8eca95bf1c",
   "metadata": {},
   "source": [
    "## 4. Save best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5b8d2-ba6d-4bbe-bb07-4d1381b9fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reformat_model_for_dataframe(model: ChosenModel) -> Dict[str, Union[str, float]]:\n",
    "#     model_dict: Dict[str, Union[str, float]] = {\n",
    "#         \"name\": model.path.as_posix().split(os.sep)[-1],\n",
    "#         \"main_metric\": model.main_metric,\n",
    "#     }\n",
    "#     model_dict.update(\n",
    "#         **{metric: value for metric, value in model.performance_metrics.items()},\n",
    "#         path=model.path.as_posix(),\n",
    "#     )\n",
    "#     return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ca249-e3f2-451f-93d7-a7d19bc60684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models_filename: str = \"best_models.csv\"\n",
    "\n",
    "# models_dataframe_list: List[Dict[str, Union[str, float]]] = [\n",
    "#     reformat_model_for_dataframe(model=model) for model in best_models.values()\n",
    "# ]\n",
    "# models_dataframe_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26ed38-fe54-4b98-b4f0-2920fe042e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models_df = pd.DataFrame(models_dataframe_list)\n",
    "# best_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125ace7-ed5f-40f1-941c-a7f4a94535c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models_df.to_csv(path_or_buf=Path(best_models_filename), index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
